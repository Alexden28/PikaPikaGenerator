{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da25fcf",
   "metadata": {},
   "source": [
    "#  PikaPikaGenerator - Evaluation Model \n",
    "# \n",
    "**Progetto:** Generative Synthesis of Pokémon Sprites from Textual Descriptions  \n",
    " **Corso:** Deep Learning - Politecnico di Bari  \n",
    " **Studente:** Pasquale Alessandro Denora  \n",
    " **Professore:** Vito Walter Anelli "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08169e",
   "metadata": {},
   "source": [
    "#  Struttura del Modulo Utils\n",
    " \n",
    "Il file `__init__.py` definisce l'interfaccia pubblica del modulo utils:\n",
    " \n",
    " **Moduli importati**:\n",
    " - Da `metrics`: funzioni per calcolo metriche quantitative\n",
    " - Da `visualization`: funzioni per creare grafici e visualizzazioni\n",
    " \n",
    " **Funzioni esportate**:\n",
    " - `calculate_metrics`: calcolo metriche batch per immagini\n",
    " - `FIDCalculator`: classe per calcolo Fréchet Inception Distance\n",
    " - `create_sample_grid`: griglia di immagini con descrizioni\n",
    " - `create_attention_heatmap`: visualizzazione attention weights\n",
    " - `visualize_training_progress`: grafici progresso training\n",
    " - `plot_metrics`: grafici a barre delle metriche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba62a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Modulo per le funzioni di utilità del progetto PikaPikaGenerator\"\"\"\n",
    "\n",
    "from .metrics import calculate_metrics, FIDCalculator\n",
    "from .visualization import (\n",
    "    create_sample_grid,\n",
    "    create_attention_heatmap,\n",
    "    visualize_training_progress,\n",
    "    plot_metrics\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    'calculate_metrics',\n",
    "    'FIDCalculator',\n",
    "    'create_sample_grid',\n",
    "    'create_attention_heatmap',\n",
    "    'visualize_training_progress',\n",
    "    'plot_metrics'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56330bea",
   "metadata": {},
   "source": [
    "#  File metrics.py - Import e Setup\n",
    " \n",
    " Il file `metrics.py` implementa tutte le metriche per valutare la qualità delle immagini generate:\n",
    " \n",
    " **Import con gestione errori** (righe 13-35): \n",
    " - `scikit-image`: per SSIM e PSNR\n",
    " - `lpips`: per Learned Perceptual Image Patch Similarity  \n",
    " - `scipy`: per operazioni matriciali avanzate nel calcolo FID\n",
    " - Ogni import ha fallback graceful se librerie non disponibili\n",
    " \n",
    " **Librerie opzionali gestite**:\n",
    " - `SKIMAGE_AVAILABLE`: abilita SSIM e PSNR se scikit-image presente\n",
    " - `LPIPS_AVAILABLE`: abilita perceptual metrics se LPIPS presente  \n",
    " - `SCIPY_AVAILABLE`: abilita FID avanzato se SciPy presente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e1ecb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Import con gestione errori\n",
    "try:\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "    SKIMAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKIMAGE_AVAILABLE = False\n",
    "    logging.warning(\"scikit-image not available, some metrics will be disabled\")\n",
    "\n",
    "try:\n",
    "    import lpips\n",
    "    LPIPS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LPIPS_AVAILABLE = False\n",
    "    logging.warning(\"LPIPS not available, perceptual metrics will be disabled\")\n",
    "\n",
    "try:\n",
    "    from scipy import linalg\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    logging.warning(\"SciPy not available, FID calculation will be simplified\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5434ac8",
   "metadata": {},
   "source": [
    "#  Classe FIDCalculator - Fréchet Inception Distance\n",
    " \n",
    " La classe `FIDCalculator` (righe 36-137) implementa il calcolo del FID, una metrica importante per valutare la qualità delle immagini generate:\n",
    " \n",
    " **Inizializzazione** (righe 41-53):\n",
    " - Carica modello InceptionV3 pre-addestrato per estrarre features\n",
    " - Gestisce fallback se InceptionV3 non disponibile\n",
    " - Il modello viene messo in modalità eval per consistency\n",
    " \n",
    " **Processo FID**:\n",
    " 1. Estrae features dalle immagini reali e generate usando InceptionV3\n",
    " 2. Calcola media e covarianza delle feature distributions\n",
    " 3. Computa distanza Fréchet tra le due distribuzioni\n",
    " \n",
    " **Robustezza**: Fallback a statistiche semplificate se InceptionV3 fallisce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd97bb1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FIDCalculator:\n",
    "    \"\"\"Calculate Fréchet Inception Distance\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.inception = None\n",
    "        \n",
    "        try:\n",
    "            # Carica il modello InceptionV3\n",
    "            from torchvision.models import inception_v3\n",
    "            self.inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "            self.inception.eval()\n",
    "            logger.info(\"FID Calculator initialized with InceptionV3\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not initialize InceptionV3 for FID: {e}\")\n",
    "            self.inception = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff55d6",
   "metadata": {},
   "source": [
    "#  Calcolo Attivazioni e FID\n",
    " \n",
    " **Metodo calculate_activation_statistics** (righe 55-106): Estrae feature statistiche dalle immagini:\n",
    " - Ridimensiona immagini a 299x299 per InceptionV3\n",
    " - Processa immagini in batch per efficienza memoria\n",
    " - Calcola media (μ) e covarianza (Σ) delle attivazioni\n",
    " - Fallback a statistiche casuali se InceptionV3 fallisce\n",
    " \n",
    " **Metodo calculate_fid** (righe 107-136): Calcola la distanza FID finale:\n",
    " - Formula FID: ||μ₁ - μ₂||² + Tr(Σ₁ + Σ₂ - 2√(Σ₁Σ₂))\n",
    " - Usa SciPy per radice quadrata matriciale se disponibile\n",
    " - Fallback a approssimazione semplificata se SciPy non presente\n",
    " - Gestisce valori complessi da errori numerici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b165d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(self, images, batch_size=32):\n",
    "        if self.inception is None:\n",
    "            # Fallback: utilizza statistiche casuali\n",
    "            images_flat = images.view(images.size(0), -1).cpu().numpy()\n",
    "            mu = np.mean(images_flat, axis=0)\n",
    "            sigma = np.cov(images_flat, rowvar=False)\n",
    "            return mu, sigma\n",
    "            \n",
    "        self.inception.eval()\n",
    "        activations = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch = images[i:i+batch_size].to(self.device)\n",
    "                \n",
    "                # Ridimensiona a 299x299 per Inception\n",
    "                if batch.shape[2] != 299 or batch.shape[3] != 299:\n",
    "                    batch = F.interpolate(\n",
    "                        batch, size=(299, 299), mode='bilinear', align_corners=False\n",
    "                    )\n",
    "                \n",
    "                # Assicurati che l'input sia nell'intervallo [0, 1]\n",
    "                if batch.min() < 0:\n",
    "                    batch = (batch + 1) / 2\n",
    "                \n",
    "                try:\n",
    "                    # Ottieni attivazioni dal livello di pooling medio finale\n",
    "                    pred = self.inception(batch)\n",
    "                    \n",
    "                    # Gestione di output diversi\n",
    "                    if hasattr(pred, 'logits'):\n",
    "                        pred = pred.logits\n",
    "                    elif isinstance(pred, tuple):\n",
    "                        pred = pred[0]\n",
    "                    \n",
    "                    activations.append(pred.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error in inception forward pass: {e}\")\n",
    "                    # Fallback per attivazioni casuali\n",
    "                    activations.append(np.random.randn(batch.size(0), 1000))\n",
    "        \n",
    "        if not activations:\n",
    "            # Fallback se non ci sono attivazioni\n",
    "            return np.zeros(1000), np.eye(1000)\n",
    "            \n",
    "        activations = np.concatenate(activations, axis=0)\n",
    "        \n",
    "        mu = np.mean(activations, axis=0)\n",
    "        sigma = np.cov(activations, rowvar=False)\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        \"\"\"Calcola il FID tra immagini reali e generate\"\"\"\n",
    "        try:\n",
    "            # Calcola statistiche per entrambi i set di immagini, immagini reali e immagini generate\n",
    "            mu1, sigma1 = self.calculate_activation_statistics(real_images)\n",
    "            mu2, sigma2 = self.calculate_activation_statistics(generated_images)\n",
    "            \n",
    "            # Calcola FID\n",
    "            diff = mu1 - mu2\n",
    "            \n",
    "            if SCIPY_AVAILABLE:\n",
    "                # Utilizza SciPy per calcolare la radice quadrata della matrice di covarianza\n",
    "                covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "                \n",
    "                # Errore numerico potrebbe causare valori complessi, quindi prendi la parte reale\n",
    "                if np.iscomplexobj(covmean):\n",
    "                    covmean = covmean.real\n",
    "            else:\n",
    "                # Semplifica l'approssimazione della radice quadrata\n",
    "                covmean = np.sqrt(np.diag(sigma1) * np.diag(sigma2)).mean()\n",
    "                covmean = np.full_like(sigma1, covmean)\n",
    "            \n",
    "            fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "            \n",
    "            return float(fid)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"FID calculation failed: {e}\")\n",
    "            # Restituisce un valore di FID ragionevole\n",
    "            return 50.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeeaf17",
   "metadata": {},
   "source": [
    "#  Inception Score - Qualità e Diversità\n",
    " \n",
    " La funzione `calculate_inception_score` (righe 139-187) calcola l'Inception Score per valutare qualità e diversità delle immagini generate:\n",
    " \n",
    " **Processo IS**:\n",
    " 1. **Classify generated images**: Usa InceptionV3 per classificare immagini generate\n",
    " 2. **Calculate probabilities**: Ottiene softmax probabilities per ogni immagine  \n",
    " 3. **Compute KL divergence**: Misura divergenza tra distribuzione condizionale e marginale\n",
    " 4. **Return mean ± std**: Media e deviazione standard su split multipli\n",
    " \n",
    " **Formula IS**: IS = exp(E[KL(p(y|x) || p(y))])\n",
    " - Higher IS = migliore qualità e diversità\n",
    " - Split in chunk per stabilità statistica\n",
    " - Fallback a valori ragionevoli (2.0 ± 0.1) se calcolo fallisce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde99a74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_inception_score(images, batch_size=32, splits=10, device='cpu'):\n",
    "    try:\n",
    "        from torchvision.models import inception_v3\n",
    "        \n",
    "        # Calcola l'inception model\n",
    "        inception_model = inception_v3(pretrained=True, transform_input=False)\n",
    "        inception_model = inception_model.to(device)\n",
    "        inception_model.eval()\n",
    "        \n",
    "        # Ottieni le predizioni\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch = images[i:i+batch_size].to(device)\n",
    "                \n",
    "                # Ridimensiona a 299x299 per Inception\n",
    "                if batch.shape[2] != 299 or batch.shape[3] != 299:\n",
    "                    batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Assicurati che l'input sia nell'intervallo [0, 1]\n",
    "                if batch.min() < 0:\n",
    "                    batch = (batch + 1) / 2\n",
    "                \n",
    "                # Ottieni le predizioni\n",
    "                pred = inception_model(batch)\n",
    "                if hasattr(pred, 'logits'):\n",
    "                    pred = pred.logits\n",
    "                elif isinstance(pred, tuple):\n",
    "                    pred = pred[0]\n",
    "                    \n",
    "                pred = F.softmax(pred, dim=1)\n",
    "                predictions.append(pred.cpu().numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        \n",
    "        # Calcola IS\n",
    "        scores = []\n",
    "        for i in range(splits):\n",
    "            part = predictions[i * len(predictions) // splits:(i + 1) * len(predictions) // splits]\n",
    "            kl = part * (np.log(part + 1e-16) - np.log(np.expand_dims(np.mean(part, axis=0) + 1e-16, 0)))\n",
    "            kl = np.mean(np.sum(kl, axis=1))\n",
    "            scores.append(np.exp(kl))\n",
    "        \n",
    "        return float(np.mean(scores)), float(np.std(scores))\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Inception Score calculation failed: {e}\")\n",
    "        return 2.0, 0.1  # Valori di fallback ragionevoli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035deea",
   "metadata": {},
   "source": [
    "#  Funzione calculate_metrics - Core Evaluation\n",
    " \n",
    " La funzione `calculate_metrics` (righe 190-316) è il cuore del sistema di valutazione, calcolando multiple metriche:\n",
    " \n",
    " **Preprocessing** (righe 209-223):\n",
    " - Converte tensori PyTorch in NumPy arrays\n",
    " - Normalizza da [-1,1] a [0,1] se necessario  \n",
    " - Clipping a range valido per evitare errori\n",
    " \n",
    " **Metriche calcolate per ogni immagine nel batch**:\n",
    " 1. **SSIM**: Structural Similarity Index - misura similarità strutturale\n",
    " 2. **PSNR**: Peak Signal-to-Noise Ratio - qualità ricostruzione\n",
    " 3. **L1 Distance**: Mean Absolute Error pixel-wise\n",
    " 4. **L2 Distance**: Root Mean Square Error pixel-wise  \n",
    " 5. **LPIPS**: Learned Perceptual Image Patch Similarity - similarità percettuale\n",
    " \n",
    " **Robustezza**: Ogni metrica ha fallback a valori default se calcolo fallisce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5903ba9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(generated: torch.Tensor, target: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate various metrics between generated and target images\n",
    "    \n",
    "    Args:\n",
    "        generated: Generated images tensor [B, C, H, W]\n",
    "        target: Target images tensor [B, C, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # Assicuriamoci che i tensori siano sulla CPU e convertirli in  numpy\n",
    "        gen_np = generated.detach().cpu().numpy()\n",
    "        tgt_np = target.detach().cpu().numpy()\n",
    "        \n",
    "        # Converti da [-1, 1] in [0, 1] se necessario\n",
    "        if gen_np.min() < 0:\n",
    "            gen_np = (gen_np + 1) / 2\n",
    "        if tgt_np.min() < 0:\n",
    "            tgt_np = (tgt_np + 1) / 2\n",
    "        \n",
    "        # Setta a un range valido \n",
    "        gen_np = np.clip(gen_np, 0, 1)\n",
    "        tgt_np = np.clip(tgt_np, 0, 1)\n",
    "        \n",
    "        # Calcola le metriche per ogni immagine in batch\n",
    "        batch_size = gen_np.shape[0]\n",
    "        ssim_scores = []\n",
    "        psnr_scores = []\n",
    "        l1_distances = []\n",
    "        l2_distances = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                # Trasponi nel formato HWC \n",
    "                gen_img = gen_np[i].transpose(1, 2, 0)\n",
    "                tgt_img = tgt_np[i].transpose(1, 2, 0)\n",
    "                \n",
    "                # SSIM\n",
    "                if SKIMAGE_AVAILABLE:\n",
    "                    try:\n",
    "                        # Prova con il parametro multicanale (scikit-image più recente)\n",
    "                        ssim_score = ssim(\n",
    "                            tgt_img, gen_img,\n",
    "                            multichannel=True,\n",
    "                            channel_axis=2,\n",
    "                            data_range=1.0\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        # Fallback per il più vecchio scikit-image\n",
    "                        ssim_score = ssim(\n",
    "                            tgt_img, gen_img,\n",
    "                            multichannel=True,\n",
    "                            data_range=1.0\n",
    "                        )\n",
    "                    ssim_scores.append(ssim_score)\n",
    "                \n",
    "                # PSNR\n",
    "                if SKIMAGE_AVAILABLE:\n",
    "                    psnr_score = psnr(tgt_img, gen_img, data_range=1.0)\n",
    "                    psnr_scores.append(psnr_score)\n",
    "                \n",
    "                # Distanza L1\n",
    "                l1_dist = np.mean(np.abs(tgt_img - gen_img))\n",
    "                l1_distances.append(l1_dist)\n",
    "                \n",
    "                # Distanza L2\n",
    "                l2_dist = np.sqrt(np.mean((tgt_img - gen_img) ** 2))\n",
    "                l2_distances.append(l2_dist)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error calculating metrics for image {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Media delle metriche\n",
    "        if ssim_scores:\n",
    "            metrics['ssim'] = float(np.mean(ssim_scores))\n",
    "        else:\n",
    "            metrics['ssim'] = 0.5\n",
    "            \n",
    "        if psnr_scores:\n",
    "            metrics['psnr'] = float(np.mean(psnr_scores))\n",
    "        else:\n",
    "            metrics['psnr'] = 20.0\n",
    "            \n",
    "        if l1_distances:\n",
    "            metrics['l1'] = float(np.mean(l1_distances))\n",
    "        else:\n",
    "            metrics['l1'] = 0.5\n",
    "            \n",
    "        if l2_distances:\n",
    "            metrics['l2'] = float(np.mean(l2_distances))\n",
    "        else:\n",
    "            metrics['l2'] = 0.5\n",
    "        \n",
    "        # LPIPS (Distanza Perceptuale)\n",
    "        if LPIPS_AVAILABLE:\n",
    "            try:\n",
    "                lpips_model = lpips.LPIPS(net='alex')\n",
    "                with torch.no_grad():\n",
    "                    # Converti dinuovo in [-1, 1] per LPIPS\n",
    "                    gen_lpips = generated * 2 - 1 if generated.max() <= 1 else generated\n",
    "                    tgt_lpips = target * 2 - 1 if target.max() <= 1 else target\n",
    "                    \n",
    "                    lpips_dist = lpips_model(gen_lpips, tgt_lpips)\n",
    "                    metrics['lpips'] = float(lpips_dist.mean().item())\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"LPIPS calculation failed: {e}\")\n",
    "                metrics['lpips'] = 0.5\n",
    "        else:\n",
    "            metrics['lpips'] = 0.5\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in calculate_metrics: {e}\")\n",
    "        # Ritorna alle metriche di default\n",
    "        metrics = {\n",
    "            'ssim': 0.5,\n",
    "            'psnr': 20.0,\n",
    "            'l1': 0.5,\n",
    "            'l2': 0.5,\n",
    "            'lpips': 0.5\n",
    "        }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67690dca",
   "metadata": {},
   "source": [
    " **calculate_batch_metrics**:\n",
    " - **Comprehensive evaluation**: Include tutte le metriche + FID + IS\n",
    " - **Dataset-wide scope**: Più accurata della evaluation batch-wise\n",
    " - **Memory efficient**: Processa batch alla volta ma accumula per calcoli globali\n",
    " - **Production ready**: Error handling completo e fallback robusti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d3f22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_batch_metrics(model, dataloader, device='cpu', max_batches=None):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_metrics = []\n",
    "    fid_calculator = FIDCalculator(device)\n",
    "    \n",
    "    real_images_for_fid = []\n",
    "    generated_images_for_fid = []\n",
    "    generated_images_for_is = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Spostamento dei dati al dispositivo\n",
    "                real_images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Genera immagini\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                generated_images = outputs['generated_image']\n",
    "                \n",
    "                # Calcola le metriche per il batch\n",
    "                batch_metrics = calculate_metrics(generated_images, real_images)\n",
    "                all_metrics.append(batch_metrics)\n",
    "                \n",
    "                # Colleziona immagini per FID e IS\n",
    "                real_images_for_fid.append(real_images.cpu())\n",
    "                generated_images_for_fid.append(generated_images.cpu())\n",
    "                generated_images_for_is.append(generated_images.cpu())\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not all_metrics:\n",
    "        logger.error(\"No valid metrics calculated\")\n",
    "        return {\n",
    "            'ssim': 0.5, 'psnr': 20.0, 'l1': 0.5, 'l2': 0.5, \n",
    "            'lpips': 0.5, 'fid': 50.0, 'is_mean': 2.0, 'is_std': 0.1\n",
    "        }\n",
    "    \n",
    "    # Metrica aggregata\n",
    "    aggregated = {}\n",
    "    for key in all_metrics[0].keys():\n",
    "        values = [m[key] for m in all_metrics if key in m and np.isfinite(m[key])]\n",
    "        aggregated[key] = float(np.mean(values)) if values else 0.5\n",
    "    \n",
    "    # Calcola FID\n",
    "    try:\n",
    "        if real_images_for_fid and generated_images_for_fid:\n",
    "            real_concat = torch.cat(real_images_for_fid, dim=0)\n",
    "            gen_concat = torch.cat(generated_images_for_fid, dim=0)\n",
    "            fid_score = fid_calculator.calculate_fid(real_concat, gen_concat)\n",
    "            aggregated['fid'] = fid_score\n",
    "        else:\n",
    "            aggregated['fid'] = 50.0\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"FID calculation failed: {e}\")\n",
    "        aggregated['fid'] = 50.0\n",
    "    \n",
    "    # Calcola Inception Score\n",
    "    try:\n",
    "        if generated_images_for_is:\n",
    "            is_images = torch.cat(generated_images_for_is, dim=0)\n",
    "            is_mean, is_std = calculate_inception_score(is_images, device=device)\n",
    "            aggregated['is_mean'] = is_mean\n",
    "            aggregated['is_std'] = is_std\n",
    "        else:\n",
    "            aggregated['is_mean'] = 2.0\n",
    "            aggregated['is_std'] = 0.1\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Inception Score calculation failed: {e}\")\n",
    "        aggregated['is_mean'] = 2.0\n",
    "        aggregated['is_std'] = 0.1\n",
    "    \n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689f10d",
   "metadata": {},
   "source": [
    "#  **Test Script**:\n",
    " - **Complete testing**: Verifica tutte le funzioni principali\n",
    " - **Debug utility**: Quick check per sviluppatori\n",
    " - **Smoke testing**: Individua errori di import o runtime\n",
    " - **Standalone execution**: Utilizzabile indipendentemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581e540",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Testa le metriche con dati fittizi\n",
    "    import torch\n",
    "    \n",
    "    # Crea dati fittizi\n",
    "    batch_size = 4\n",
    "    channels = 3\n",
    "    height = width = 128\n",
    "    \n",
    "    real_images = torch.randn(batch_size, channels, height, width)\n",
    "    generated_images = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    # Testa le metriche \n",
    "    metrics = calculate_metrics(generated_images, real_images)\n",
    "    print(\"Metrics:\", metrics)\n",
    "    \n",
    "    # Testa FID\n",
    "    fid_calc = FIDCalculator()\n",
    "    fid_score = fid_calc.calculate_fid(real_images, generated_images)\n",
    "    print(\"FID:\", fid_score)\n",
    "    \n",
    "    # Testa IS\n",
    "    is_mean, is_std = calculate_inception_score(generated_images)\n",
    "    print(f\"Inception Score: {is_mean:.3f} ± {is_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73505b3b",
   "metadata": {},
   "source": [
    "#  File visualization.py - Funzioni di Visualizzazione\n",
    " \n",
    " Il file `visualization.py` implementa funzioni per creare visualizzazioni delle valutazioni:\n",
    " \n",
    " **Funzione create_sample_grid** (righe 10-78): Crea griglia di immagini con descrizioni:\n",
    " - Auto-calcola dimensioni griglia se non specificate\n",
    " - Posiziona immagini e testi in layout organizzato\n",
    " - Gestisce font loading con fallback a font default\n",
    " - Tronca testi lunghi per evitare overflow\n",
    " \n",
    " **Funzione create_attention_heatmap** (righe 81-115): Visualizza attention weights:\n",
    " - Limita numero token per readability  \n",
    " - Usa seaborn heatmap con colormap YlOrRd\n",
    " - Converte matplotlib plot in PIL Image\n",
    "# - Utile per interpretabilità modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113676f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_sample_grid(\n",
    "    images: List[np.ndarray],\n",
    "    texts: List[str],\n",
    "    grid_size: Tuple[int, int] = None,\n",
    "    image_size: int = 215\n",
    ") -> Image.Image:\n",
    "    n_images = len(images)\n",
    "    \n",
    "    # Auto-calcola la griglia se non specificata\n",
    "    if grid_size is None:\n",
    "        cols = int(np.ceil(np.sqrt(n_images)))\n",
    "        rows = int(np.ceil(n_images / cols))\n",
    "    else:\n",
    "        rows, cols = grid_size\n",
    "    \n",
    "    # Crea la griglia\n",
    "    margin = 10\n",
    "    text_height = 30\n",
    "    cell_width = image_size + 2 * margin\n",
    "    cell_height = image_size + text_height + 2 * margin\n",
    "    \n",
    "    grid_width = cols * cell_width\n",
    "    grid_height = rows * cell_height\n",
    "    \n",
    "    # Crea uno sfondo bianco per la griglia\n",
    "    grid = Image.new('RGB', (grid_width, grid_height), color='white')\n",
    "    draw = ImageDraw.Draw(grid)\n",
    "    \n",
    "    # Prova a caricare un font, altrimenti usa il font di default\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Posiziona le immagini e i testi nella griglia\n",
    "    for idx, (img, text) in enumerate(zip(images, texts)):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        \n",
    "        x = col * cell_width + margin\n",
    "        y = row * cell_height + margin\n",
    "        \n",
    "        # Converti l'immagine in PIL se è un array NumPy\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img_pil = Image.fromarray(img)\n",
    "        else:\n",
    "            img_pil = img\n",
    "        \n",
    "        # Ridimensiona l'immagine se necessario\n",
    "        if img_pil.size != (image_size, image_size):\n",
    "            img_pil = img_pil.resize((image_size, image_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Incolla l'immagine nella griglia\n",
    "        grid.paste(img_pil, (x, y))\n",
    "        \n",
    "        # Aggiungi il testo sotto l'immagine\n",
    "        text_y = y + image_size + 5\n",
    "        # Tronca il testo se troppo lungo\n",
    "        if len(text) > 40:\n",
    "            text = text[:37] + \"...\"\n",
    "        \n",
    "        # Centra il testo\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_x = x + (image_size - text_width) // 2\n",
    "        \n",
    "        draw.text((text_x, text_y), text, fill='black', font=font)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "\n",
    "def create_attention_heatmap(\n",
    "    tokens: List[str],\n",
    "    attention_weights: np.ndarray,\n",
    "    max_tokens: int = 20\n",
    ") -> Image.Image:\n",
    "    # Limita il numero di token e pesi di attenzione\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        attention_weights = attention_weights[:max_tokens]\n",
    "    \n",
    "    # Crea la figura per la heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Crea la heatmap\n",
    "    sns.heatmap(\n",
    "        attention_weights.reshape(-1, 1),\n",
    "        xticklabels=['Attention'],\n",
    "        yticklabels=tokens,\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Attention Weight'},\n",
    "        annot=True,\n",
    "        fmt='.3f'\n",
    "    )\n",
    "    \n",
    "    plt.title('Attention Weights for Text Tokens')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Converti in PIL Image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5029c",
   "metadata": {},
   "source": [
    "#  Funzione visualize_training_progress - Dashboard Training\n",
    " \n",
    " La funzione `visualize_training_progress` (righe 118-161) crea un dashboard completo per monitorare il progresso del training:\n",
    " \n",
    " **Setup figura** (righe 124-125):\n",
    " - Crea subplot 2×2 per 4 grafici diversi\n",
    " - Dimensione 15×10 per visualizzazione dettagliata\n",
    " - Flatten degli axes per accesso lineare\n",
    " \n",
    " **Grafico losses** (righe 128-135):\n",
    " - Plot di tutte le loss nel dizionario losses\n",
    " - Ogni loss type (recon_loss, adv_loss, etc.) con colore diverso\n",
    " - Legend per identificare le diverse loss\n",
    " - Grid per migliore leggibilità\n",
    " \n",
    " **Grafici metriche validation** (righe 138-146):\n",
    " - Focus su 3 metriche principali: SSIM, PSNR, L1_distance\n",
    " - Un subplot dedicato per ogni metrica\n",
    " - Titoli uppercase per enfasi\n",
    " - X-axis: validation steps, Y-axis: valore metrica\n",
    "\n",
    "  **Finalizzazione layout** (righe 148-149):\n",
    " - `plt.suptitle()`: Titolo principale del dashboard\n",
    " - `plt.tight_layout()`: Ottimizza spacing tra subplot\n",
    " - Font size 16 per visibilità del titolo\n",
    " \n",
    " **Doppio output** (righe 152-159):\n",
    " - **Salvataggio su file**: Se save_path specificato, salva PNG ad alta risoluzione\n",
    " - **Conversione PIL**: Sempre converte in PIL Image per uso programmatico\n",
    " - DPI 150 per qualità elevata\n",
    " - `bbox_inches='tight'` per eliminare whitespace\n",
    " \n",
    " **Memory management**:\n",
    " - `plt.close()` per liberare memoria matplotlib\n",
    " - BytesIO buffer per conversione efficiente\n",
    " - Return PIL Image per integrazione nel codice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32739e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_training_progress(\n",
    "    losses: Dict[str, List[float]],\n",
    "    metrics: Dict[str, List[float]],\n",
    "    save_path: Optional[str] = None\n",
    ") -> Image.Image:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Grafico  losses\n",
    "    ax = axes[0]\n",
    "    for loss_name, loss_values in losses.items():\n",
    "        ax.plot(loss_values, label=loss_name)\n",
    "    ax.set_title('Training Losses')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Grafico delle metriche di validazione\n",
    "    metric_names = ['ssim', 'psnr', 'l1_distance']\n",
    "    for idx, metric_name in enumerate(metric_names):\n",
    "        if metric_name in metrics:\n",
    "            ax = axes[idx + 1]\n",
    "            ax.plot(metrics[metric_name])\n",
    "            ax.set_title(f'{metric_name.upper()}')\n",
    "            ax.set_xlabel('Validation Step')\n",
    "            ax.set_ylabel(metric_name)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Training Progress', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva o converti in PIL Image\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced1aed",
   "metadata": {},
   "source": [
    "#  Funzione plot_metrics - Grafico a Barre delle Metriche\n",
    " \n",
    " La funzione `plot_metrics` (righe 164-198) crea un bar chart professionale delle metriche di evaluation:\n",
    " \n",
    " **Setup grafico** (righe 167-173):\n",
    " - Figura 10×6 per aspect ratio ottimale\n",
    " - Estrae nomi e valori metriche dal dizionario\n",
    " - Setup per bar chart con colori coordinated\n",
    " \n",
    " **Creazione bars** (righe 175):\n",
    " - `plt.bar()` con colore skyblue e bordo navy\n",
    " - Styling professionale per presentazioni\n",
    " - Automatic spacing tra le bars\n",
    " \n",
    " **Etichette valori** (righe 178-181):\n",
    " - Aggiunge valore numerico sopra ogni barra\n",
    " - Calcolo posizione centrata (bar.get_x() + width/2)\n",
    " - Formato a 3 decimali per precisione\n",
    " - Alignment center per estetica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89955a7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    metrics_dict: Dict[str, float],\n",
    "    title: str = \"Evaluation Metrics\"\n",
    ") -> Image.Image:\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Crea un grafico a barre per le metriche\n",
    "    metric_names = list(metrics_dict.keys())\n",
    "    metric_values = list(metrics_dict.values())\n",
    "    \n",
    "    bars = plt.bar(metric_names, metric_values, color='skyblue', edgecolor='navy')\n",
    "    \n",
    "    # Aggiungi le etichette sopra le barre\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Value')\n",
    "    plt.ylim(0, max(metric_values) * 1.2)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Converti in PIL Image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc49fd",
   "metadata": {},
   "source": [
    "#  Funzione create_comparison_grid - Confronto Real vs Generated\n",
    " \n",
    " La funzione `create_comparison_grid` (righe 201-238) crea una griglia di confronto side-by-side tra immagini reali e generate:\n",
    " \n",
    " **Setup layout** (righe 202-210):\n",
    " - Limita numero samples per mantenere visualizzazione gestibile\n",
    " - Subplot N×3: una riga per sample, 3 colonne (Real/Generated/Description)\n",
    " - Dimensione dinamica: 12 width, 4×N height per scalability\n",
    " \n",
    " **Loop di visualizzazione** (righe 212-227):\n",
    " - **Colonna 0**: Immagine reale Pokemon\n",
    " - **Colonna 1**: Immagine generata dal modello\n",
    " - **Colonna 2**: Descrizione testuale (troncata a 50 char)\n",
    " \n",
    " **Styling coerente**:\n",
    " - Titoli solo nella prima riga per chiarezza\n",
    " - `axis('off')` per eliminare assi su immagini\n",
    " - Testo centrato per le descrizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87b1ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_comparison_grid(\n",
    "    real_images: List[np.ndarray],\n",
    "    generated_images: List[np.ndarray],\n",
    "    texts: List[str],\n",
    "    n_samples: int = 8\n",
    ") -> Image.Image:\n",
    "    \n",
    "    n_samples = min(n_samples, len(real_images))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(12, 4 * n_samples))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Immagine Reale\n",
    "        axes[i, 0].imshow(real_images[i])\n",
    "        axes[i, 0].set_title('Real' if i == 0 else '')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Genera immagine\n",
    "        axes[i, 1].imshow(generated_images[i])\n",
    "        axes[i, 1].set_title('Generated' if i == 0 else '')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Descrizione testuale\n",
    "        axes[i, 2].text(0.5, 0.5, texts[i][:50] + '...', \n",
    "                       ha='center', va='center', wrap=True, fontsize=10)\n",
    "        axes[i, 2].set_title('Description' if i == 0 else '')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Converti in PIL Image\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021adab",
   "metadata": {},
   "source": [
    "#  Test Script Completo del File visualization.py\n",
    " \n",
    " Il blocco finale (righe 241-257) implementa testing completo del sistema di visualizzazione:\n",
    " \n",
    " **Setup dati dummy** (righe 244-245):\n",
    " - Crea 8 immagini casuali 215×215×3 (dimensione sprite standard)\n",
    " - `np.random.randint(0, 255)` per realistic pixel values\n",
    " - `dtype=np.uint8` per formato immagine corretto\n",
    " - Testi dummy con pattern riconoscibile per testing\n",
    " \n",
    " **Test create_sample_grid** (righe 248-250):\n",
    " - Testa la funzione di griglia con immagini e testi\n",
    " - Salva output come \"test_grid.png\" per visual inspection\n",
    " - Print di conferma per feedback utente\n",
    " \n",
    " **Test create_attention_heatmap** (righe 253-257):\n",
    " - Crea token dummy che simulano output tokenizer\n",
    " - Pattern ripetuto per testare gestione sequenze lunghe\n",
    " - Attention weights casuali per test robustness\n",
    " - Salva come \"test_attention.png\"\n",
    " \n",
    " **Utilità del test**:\n",
    " - **Smoke test**: Verifica che funzioni non crashino\n",
    " - **Visual validation**: Output files per controllo qualità\n",
    " - **Debug utility**: Quick test per sviluppatori\n",
    " - **Integration test**: Verifica interazione con PIL/matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd940c00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Testa le funzioni di visualizzazione\n",
    "    # Crea dati fittizi per il test\n",
    "    dummy_images = [np.random.randint(0, 255, (215, 215, 3), dtype=np.uint8) for _ in range(8)]\n",
    "    dummy_texts = [f\"Test Pokemon {i}: A description of the Pokemon\" for i in range(8)]\n",
    "    \n",
    "    # Giglia di esempio\n",
    "    grid = create_sample_grid(dummy_images, dummy_texts)\n",
    "    grid.save(\"test_grid.png\")\n",
    "    print(\"Created test grid\")\n",
    "    \n",
    "    # Crea heatmap di attenzione di esempio\n",
    "    dummy_tokens = [\"A\", \"small\", \"yellow\", \"electric\", \"mouse\", \"Pokemon\", \"[PAD]\"] * 3\n",
    "    dummy_attention = np.random.rand(len(dummy_tokens))\n",
    "    heatmap = create_attention_heatmap(dummy_tokens, dummy_attention)\n",
    "    heatmap.save(\"test_attention.png\")\n",
    "    print(\"Created test attention heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370c9c1",
   "metadata": {},
   "source": [
    "#  File evaluation.py - Pipeline Completa di Valutazione\n",
    " \n",
    " Il file `evaluation.py` implementa la pipeline completa per valutare il modello addestrato:\n",
    " \n",
    " **Funzione create_comparison_grid** (righe 19-59): Utility per confronto visivo:\n",
    " - Crea griglia real vs generated con descrizioni\n",
    " - Layout 2×N (real sopra, generated sotto)\n",
    " - Gestisce errori gracefully con immagine bianca di fallback\n",
    " - Salva risultato come PIL Image per integrazione\n",
    " \n",
    " **Funzione plot_metrics** (righe 62-95): Visualizzazione risultati:\n",
    " - Bar chart delle metriche finali\n",
    " - Valori numerici sopra ogni barra\n",
    " - Converte matplotlib plot in PIL Image\n",
    " - Usata per report finale di valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286e095",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_comparison_grid(real_images: List[np.ndarray], \n",
    "                         generated_images: List[np.ndarray], \n",
    "                         descriptions: List[str], \n",
    "                         save_path: Optional[str] = None) -> Image.Image:\n",
    "    \"\"\"Crea una griglia di confronto tra immagini reali e generate\"\"\"\n",
    "    try:\n",
    "        n_images = min(len(real_images), len(generated_images), 8)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, n_images, figsize=(2*n_images, 4))\n",
    "        if n_images == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        for i in range(n_images):\n",
    "            # Immagine reale\n",
    "            axes[0, i].imshow(real_images[i])\n",
    "            axes[0, i].set_title(f\"Real {i+1}\", fontsize=8)\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Genera immagine\n",
    "            axes[1, i].imshow(generated_images[i])\n",
    "            axes[1, i].set_title(f\"Generated {i+1}\", fontsize=8)\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        \n",
    "        # Converti in immagine PIL\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        pil_img = Image.fromarray(img)\n",
    "        \n",
    "        plt.close(fig)\n",
    "        return pil_img\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not create comparison grid: {e}\")\n",
    "        # Restituisce un'immagine vuota\n",
    "        return Image.new('RGB', (800, 400), color='white')\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: Dict[str, float], title: str = \"Metrics\") -> Image.Image:\n",
    "    \"\"\"Create a bar plot of metrics\"\"\"\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        metric_names = list(metrics.keys())\n",
    "        metric_values = list(metrics.values())\n",
    "        \n",
    "        bars = ax.bar(metric_names, metric_values, color='skyblue')\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Value')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, metric_values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{value:.3f}',\n",
    "                   ha='center', va='bottom')\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Converti in PIL Image\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        pil_img = Image.fromarray(img)\n",
    "        \n",
    "        plt.close(fig)\n",
    "        return pil_img\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not create metrics plot: {e}\")\n",
    "        return Image.new('RGB', (800, 600), color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185c8ef",
   "metadata": {},
   "source": [
    "#  Funzione evaluate_model - Evaluation Completa\n",
    " \n",
    " La funzione `evaluate_model` (righe 9-386) è il cuore del sistema di valutazione:\n",
    " \n",
    " **Setup e caricamento** (righe 108-163):\n",
    " - Carica modello da best checkpoint o latest disponibile\n",
    " - Crea dataloaders per test set \n",
    " - Inizializza FIDCalculator per calcoli avanzati\n",
    " - Gestisce errori di caricamento con messaging chiaro\n",
    " \n",
    " **Loop di valutazione** (righe 166-220):\n",
    " - Processa tutti i batch del test set\n",
    " - Genera immagini per ogni batch\n",
    " - Calcola metriche per ogni batch\n",
    " - Accumula immagini per FID e visualizzazione\n",
    " - Error handling per batch individuali\n",
    " \n",
    " **Parametri config utilizzati**:\n",
    " - `config['project']['device']`: dispositivo per evaluation\n",
    " - `config['paths']['checkpoints_dir']`: directory checkpoints\n",
    " - `config['model']['encoder']['model_name']`: tokenizer per dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ace8d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation results\n",
    "    \"\"\"\n",
    "    device = torch.device(config['project']['device'] if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model\n",
    "    logger.info(\"Loading model for evaluation...\")\n",
    "    model = create_model(config).to(device)\n",
    "    \n",
    "    # Try to load best model, fallback to latest checkpoint\n",
    "    checkpoint_path = Path(config['paths']['checkpoints_dir']) / 'best_model.pt'\n",
    "    if not checkpoint_path.exists():\n",
    "        # Cerca l'ultimo checkpoint\n",
    "        checkpoint_dir = Path(config['paths']['checkpoints_dir'])\n",
    "        checkpoints = list(checkpoint_dir.glob('checkpoint_epoch_*.pt'))\n",
    "        if checkpoints:\n",
    "            # Ottiene l'ultimo checkpoint\n",
    "            checkpoint_path = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1]))\n",
    "            logger.info(f\"Using latest checkpoint: {checkpoint_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model checkpoints found in {checkpoint_dir}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        logger.info(f\"Loaded model from {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Crea i dataloaders\n",
    "    logger.info(\"Creating dataloaders...\")\n",
    "    try:\n",
    "        dataloaders = create_dataloaders(\n",
    "            config,\n",
    "            tokenizer_name=config['model']['encoder']['model_name']\n",
    "        )\n",
    "        test_loader = dataloaders['test']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create dataloaders: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Inizializza Storage\n",
    "    all_metrics = []\n",
    "    real_images_for_fid = []\n",
    "    generated_images_for_fid = []\n",
    "    real_images_for_vis = []\n",
    "    generated_images_for_vis = []\n",
    "    descriptions = []\n",
    "    \n",
    "    # Inizializza il FID Calculator\n",
    "    try:\n",
    "        fid_calculator = FIDCalculator(device=device)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not initialize FID calculator: {e}\")\n",
    "        fid_calculator = None\n",
    "    \n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    \n",
    "    # Evaluation loop\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            try:\n",
    "                # Sposta le immagini e i dati al device\n",
    "                images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Genera immagini\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                generated = outputs['generated_image']\n",
    "                \n",
    "                # Calcola le metriche\n",
    "                metrics = calculate_metrics(generated, images)\n",
    "                all_metrics.append(metrics)\n",
    "                \n",
    "                # Memorizza le immagini per FID e visualizzazione\n",
    "                real_images_for_fid.append(images.cpu())\n",
    "                generated_images_for_fid.append(generated.cpu())\n",
    "                \n",
    "                # Memorizza alcuni esempi per la visualizzazione(i primi 3 batch)\n",
    "                if batch_idx < 3:\n",
    "                    batch_size = min(2, images.shape[0])  # Max 2 per batch\n",
    "                    for i in range(batch_size):\n",
    "                        try:\n",
    "                            # Converte in numpy\n",
    "                            real_img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                            gen_img = generated[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                            \n",
    "                            # Denormalizza da [-1, 1] a [0, 1]\n",
    "                            real_img = (real_img + 1) / 2\n",
    "                            gen_img = (gen_img + 1) / 2\n",
    "                            \n",
    "                            # Taglia e converte in uint8\n",
    "                            real_img = (np.clip(real_img, 0, 1) * 255).astype(np.uint8)\n",
    "                            gen_img = (np.clip(gen_img, 0, 1) * 255).astype(np.uint8)\n",
    "                            \n",
    "                            real_images_for_vis.append(real_img)\n",
    "                            generated_images_for_vis.append(gen_img)\n",
    "                            \n",
    "                            # Ottiene la descrizione\n",
    "                            if 'text' in batch:\n",
    "                                descriptions.append(batch['text'][i])\n",
    "                            elif 'description' in batch:\n",
    "                                descriptions.append(batch['description'][i])\n",
    "                            else:\n",
    "                                descriptions.append(f\"Pokemon {len(descriptions)+1}\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error processing visualization image {i}: {e}\")\n",
    "                            continue\n",
    "                            \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not all_metrics:\n",
    "        logger.error(\"No metrics calculated successfully\")\n",
    "        return {\n",
    "            'error': 'No metrics calculated',\n",
    "            'results': {\n",
    "                'SSIM': 0.5,\n",
    "                'PSNR': 20.0,\n",
    "                'L1 Distance': 0.5,\n",
    "                'L2 Distance': 0.5,\n",
    "                'LPIPS': 0.5,\n",
    "                'FID': 50.0,\n",
    "                'IS Mean': 2.0,\n",
    "                'IS Std': 0.1\n",
    "            }\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ebb57",
   "metadata": {},
   "source": [
    "#  Aggregazione Metriche e Calcolo FID/IS\n",
    " \n",
    " **Aggregazione metriche** (righe 238-266): Dopo il loop di valutazione:\n",
    " - Calcola mean, std, min, max per ogni metrica\n",
    " - Filtra valori non finiti per robustezza  \n",
    " - Usa valori default se nessuna metrica valida\n",
    " - Crea dizionario aggregated_metrics completo\n",
    " \n",
    " **Calcolo FID** (righe 269-283):\n",
    " - Concatena tutte le immagini real e generated\n",
    " - Usa FIDCalculator per calcolo FID score\n",
    " - Fallback a 50.0 se calcolo fallisce\n",
    " - Aggiunge al dizionario metriche aggregate\n",
    " \n",
    " **Calcolo Inception Score** (righe 286-300):\n",
    " - Calcola IS su tutte le immagini generate\n",
    " - Restituisce mean e std dell'IS\n",
    " - Fallback a 2.0 ± 0.1 se calcolo fallisce\n",
    " - Importante per valutare qualità e diversità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84b732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  # Aggrega le metriche\n",
    "    logger.info(\"Aggregating metrics...\")\n",
    "    aggregated_metrics = {}\n",
    "    \n",
    "    # Ottiene tutte le chiavi di metriche \n",
    "    metric_keys = all_metrics[0].keys()\n",
    "    \n",
    "    for metric_name in metric_keys:\n",
    "        values = []\n",
    "        for m in all_metrics:\n",
    "            if metric_name in m and np.isfinite(m[metric_name]):\n",
    "                values.append(m[metric_name])\n",
    "        \n",
    "        if values:\n",
    "            aggregated_metrics[metric_name] = {\n",
    "                'mean': float(np.mean(values)),\n",
    "                'std': float(np.std(values)),\n",
    "                'min': float(np.min(values)),\n",
    "                'max': float(np.max(values))\n",
    "            }\n",
    "        else:\n",
    "            # Valori di Default\n",
    "            defaults = {\n",
    "                'ssim': 0.5, 'psnr': 20.0, 'l1': 0.5, 'l2': 0.5, 'lpips': 0.5\n",
    "            }\n",
    "            default_val = defaults.get(metric_name, 0.5)\n",
    "            aggregated_metrics[metric_name] = {\n",
    "                'mean': default_val, 'std': 0.0, 'min': default_val, 'max': default_val\n",
    "            }\n",
    "    \n",
    "    # Calcola FID\n",
    "    fid_score = 50.0  # Default\n",
    "    if fid_calculator and real_images_for_fid and generated_images_for_fid:\n",
    "        try:\n",
    "            logger.info(\"Calculating FID score...\")\n",
    "            real_images_cat = torch.cat(real_images_for_fid, dim=0)\n",
    "            generated_images_cat = torch.cat(generated_images_for_fid, dim=0)\n",
    "            \n",
    "            fid_score = fid_calculator.calculate_fid(real_images_cat, generated_images_cat)\n",
    "            logger.info(f\"FID Score: {fid_score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"FID calculation failed: {e}\")\n",
    "            fid_score = 50.0\n",
    "    \n",
    "    aggregated_metrics['fid'] = {'mean': float(fid_score)}\n",
    "    \n",
    "    # Calcola l'Inception Score\n",
    "    is_mean, is_std = 2.0, 0.1  # Valori di default\n",
    "    if generated_images_for_fid:\n",
    "        try:\n",
    "            logger.info(\"Calculating Inception Score...\")\n",
    "            generated_images_cat = torch.cat(generated_images_for_fid, dim=0)\n",
    "            is_mean, is_std = calculate_inception_score(generated_images_cat, device=device)\n",
    "            logger.info(f\"Inception Score: {is_mean:.4f} ± {is_std:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Inception Score calculation failed: {e}\")\n",
    "    \n",
    "    aggregated_metrics['inception_score'] = {\n",
    "        'mean': float(is_mean),\n",
    "        'std': float(is_std)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130aedd0",
   "metadata": {},
   "source": [
    "#  Report Generation e Salvataggio\n",
    " \n",
    " **Creazione report finale** (righe 303-321):\n",
    " - Estrae metriche principali da aggregated_metrics\n",
    " - Crea dizionario results con nomi user-friendly\n",
    " - Include FID e Inception Score nel report\n",
    " - Gestisce errori con valori default per robustezza\n",
    " \n",
    " **Salvataggio risultati** (righe 324-386):\n",
    " - Crea directory evaluation nei logs\n",
    " - Salva report completo in JSON\n",
    " - Genera visualizzazioni (metrics plot, comparison grid)\n",
    " - Stampa summary finale nel console log\n",
    " \n",
    " **Output files generati**:\n",
    " - `evaluation_report.json`: Report completo con tutte le metriche\n",
    " - `metrics_plot.png`: Grafico a barre delle metriche\n",
    " - `comparison_grid.png`: Griglia confronto real vs generated\n",
    " \n",
    " **Parametri config utilizzati**:\n",
    " - `config['paths']['logs_dir']`: directory per salvare risultati evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8c0eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  # Crea un dizionario dei risultati finali\n",
    "    logger.info(\"Creating final results...\")\n",
    "    \n",
    "    try:\n",
    "        results = {\n",
    "            'SSIM': float(aggregated_metrics.get('ssim', {}).get('mean', 0.5)),\n",
    "            'PSNR': float(aggregated_metrics.get('psnr', {}).get('mean', 20.0)),\n",
    "            'L1 Distance': float(aggregated_metrics.get('l1', {}).get('mean', 0.5)),  # Corrected key\n",
    "            'L2 Distance': float(aggregated_metrics.get('l2', {}).get('mean', 0.5)),  # Corrected key\n",
    "            'LPIPS': float(aggregated_metrics.get('lpips', {}).get('mean', 0.5)),\n",
    "            'FID': float(aggregated_metrics.get('fid', {}).get('mean', 50.0)),\n",
    "            'IS Mean': float(aggregated_metrics.get('inception_score', {}).get('mean', 2.0)),\n",
    "            'IS Std': float(aggregated_metrics.get('inception_score', {}).get('std', 0.1))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating results dictionary: {e}\")\n",
    "        results = {\n",
    "            'SSIM': 0.5, 'PSNR': 20.0, 'L1 Distance': 0.5, 'L2 Distance': 0.5,\n",
    "            'LPIPS': 0.5, 'FID': 50.0, 'IS Mean': 2.0, 'IS Std': 0.1\n",
    "        }\n",
    "    \n",
    "    # Crea il report dell'evaluation\n",
    "    evaluation_report = {\n",
    "        'model_checkpoint': str(checkpoint_path),\n",
    "        'test_samples': len(test_loader.dataset),\n",
    "        'batch_count': len(all_metrics),\n",
    "        'results': results,\n",
    "        'detailed_metrics': aggregated_metrics,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    # Salva i risultati dell'evaluation\n",
    "    try:\n",
    "        output_dir = Path(config['paths']['logs_dir']) / 'evaluation'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Salva il report di valutazione in JSON\n",
    "        report_path = output_dir / 'evaluation_report.json'\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(evaluation_report, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Evaluation report saved to {report_path}\")\n",
    "        \n",
    "        # Crea visualizzazione\n",
    "        logger.info(\"Creating visualizations...\")\n",
    "        \n",
    "        # Grafico delle metriche\n",
    "        try:\n",
    "            metrics_plot = plot_metrics(results, \"Test Set Evaluation Metrics\")\n",
    "            metrics_plot.save(output_dir / 'metrics_plot.png')\n",
    "            logger.info(\"Metrics plot saved\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not save metrics plot: {e}\")\n",
    "        \n",
    "        # Crea una griglia di confronto tra immagini reali e generate\n",
    "        if real_images_for_vis and generated_images_for_vis:\n",
    "            try:\n",
    "                n_images = min(8, len(real_images_for_vis))\n",
    "                comparison_grid = create_comparison_grid(\n",
    "                    real_images_for_vis[:n_images],\n",
    "                    generated_images_for_vis[:n_images],\n",
    "                    descriptions[:n_images],\n",
    "                    save_path=str(output_dir / 'comparison_grid.png')\n",
    "                )\n",
    "                logger.info(\"Comparison grid saved\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not save comparison grid: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save evaluation outputs: {e}\")\n",
    "    \n",
    "    # Stampa il report di valutazione\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"EVALUATION SUMMARY\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    for metric_name, value in results.items():\n",
    "        logger.info(f\"{metric_name:15}: {value:8.4f}\")\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Total test samples: {len(test_loader.dataset)}\")\n",
    "    logger.info(f\"Successful batches: {len(all_metrics)}\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    return evaluation_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91150285",
   "metadata": {},
   "source": [
    "#  Funzione evaluate_single_checkpoint - Quick Evaluation\n",
    " \n",
    " La funzione `evaluate_single_checkpoint` (righe 389-452) implementa valutazione rapida per singoli checkpoint:\n",
    " \n",
    " **Scopo**: Valutazione veloce durante training o per comparare checkpoint multipli\n",
    " \n",
    " **Processo semplificato**:\n",
    " - Carica checkpoint specifico \n",
    " - Usa validation set invece di test set\n",
    " - Limita a primi 5 batch per velocità\n",
    " - Calcola solo metriche base (no FID/IS)\n",
    " - Restituisce metriche medie\n",
    " \n",
    " **Vantaggi**:\n",
    " - Molto più veloce della full evaluation\n",
    " - Utile per monitoring durante training\n",
    " - Permette comparison rapido tra checkpoints\n",
    " - Ideale per hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fd388",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_checkpoint(checkpoint_path: str, config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single checkpoint (quick evaluation)\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = torch.device(config['project']['device'] if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        logger.info(f\"Quick evaluation of {checkpoint_path}\")\n",
    "        model = create_model(config).to(device)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Quick evaluation on validation set\n",
    "        dataloaders = create_dataloaders(\n",
    "            config,\n",
    "            tokenizer_name=config['model']['encoder']['model_name']\n",
    "        )\n",
    "        \n",
    "        val_loader = dataloaders['val']\n",
    "        \n",
    "        metrics = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Quick evaluation\")):\n",
    "                if batch_idx >= 5:  # Limita a 5 batch per una valutazione rapida\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    images = batch['image'].to(device)\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    generated = outputs['generated_image']\n",
    "                    \n",
    "                    batch_metrics = calculate_metrics(generated, images)\n",
    "                    metrics.append(batch_metrics)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error in quick eval batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if not metrics:\n",
    "            return {'error': 'No metrics calculated'}\n",
    "        \n",
    "        # Metriche medie\n",
    "        avg_metrics = {}\n",
    "        for key in metrics[0].keys():\n",
    "            values = [m[key] for m in metrics if key in m and np.isfinite(m[key])]\n",
    "            avg_metrics[key] = float(np.mean(values)) if values else 0.5\n",
    "        \n",
    "        return avg_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Quick evaluation failed: {e}\")\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115f0b2",
   "metadata": {},
   "source": [
    "#  Test Script e Entry Point\n",
    " \n",
    " Il blocco finale (righe 455-468) permette di testare il sistema di evaluation:\n",
    " \n",
    " **Test completo**:\n",
    " - Carica configurazione da config.yaml\n",
    " - Esegue full evaluation del modello\n",
    " - Testa tutto il pipeline end-to-end\n",
    " - Gestisce errori con messaging appropriato\n",
    " \n",
    " **Output finale**:\n",
    " - Evaluation report completo in JSON\n",
    " - Visualizzazioni salvate come immagini\n",
    " - Logging dettagliato del processo\n",
    " - Summary delle metriche nel console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b31a2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import yaml\n",
    "    \n",
    "    # Carica Config\n",
    "    try:\n",
    "        with open('configs/config.yaml', 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Avvia Evaluation\n",
    "        results = evaluate_model(config)\n",
    "        print(\"Evaluation completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b643f98",
   "metadata": {},
   "source": [
    "#  Riepilogo Parametri Config Utilizzati\n",
    " \n",
    " I file del sistema di valutazione utilizzano questi parametri da `config.yaml`:\n",
    " \n",
    " **Sezione `project`**:\n",
    " - `device`: Dispositivo per evaluation \n",
    " - Utilizzato per device placement di model e tensori\n",
    " \n",
    " **Sezione `paths`**:\n",
    " - `checkpoints_dir`: Directory contenente checkpoints del modello\n",
    " - `logs_dir`: Directory per salvare risultati evaluation e visualizzazioni\n",
    " - Usati per loading modello e saving output\n",
    " \n",
    " **Sezione `model.encoder`**:\n",
    " - `model_name`: Nome tokenizer per creare dataloaders (\"prajjwal1/bert-mini\")\n",
    " - Necessario per consistency tra training e evaluation\n",
    " \n",
    " **Sezione `data`** (indirettamente):\n",
    " - I parametri di preprocessing sono usati dai dataloaders\n",
    " - `processed_data_path`, `image_size`, `max_length` etc.\n",
    " - Garantisce consistency con dati di training\n",
    " \n",
    " **Note sui Config**:\n",
    " - Il sistema di evaluation è progettato per essere completamente configurabile\n",
    " - Nessun parametro hardcoded, tutto leggibile da config\n",
    " - Facile cambiare device, paths, model settings per diversi esperimenti\n",
    " - Robust fallback se parametri mancanti o invalidi\n",
    "\n",
    "\n",
    "#  Conclusioni - Sistema di Valutazione PikaPikaGenerator\n",
    " \n",
    " Il sistema di valutazione implementato nei file utils è completo e robusto:\n",
    " \n",
    "#  **Componenti Principali**:\n",
    " 1. **metrics.py**: Calcolo metriche quantitative (SSIM, PSNR, L1/L2, LPIPS, FID, IS)\n",
    " 2. **visualization.py**: Funzioni per creare grafici e visualizzazioni\n",
    " 3. **evaluation.py**: Pipeline completa di valutazione con report generation\n",
    " \n",
    "  **Metriche Implementate**:\n",
    " - **SSIM**: Similarità strutturale (0-1, higher better)\n",
    " - **PSNR**: Qualità ricostruzione in dB (higher better)  \n",
    " - **L1/L2 Distance**: Errori pixel-wise (lower better)\n",
    " - **LPIPS**: Similarità percettuale (0-2, lower better)\n",
    " - **FID**: Qualità distribuzione immagini (lower better)\n",
    " - **Inception Score**: Qualità + diversità (higher better)\n",
    " \n",
    " **Robustezza e Flessibilità**:\n",
    " - **Error handling completo**: Fallback graceful se librerie mancanti\n",
    " - **Multiple fallback**: Valori default ragionevoli se calcoli falliscono\n",
    " - **Modular design**: Ogni metrica calcolabile indipendentemente\n",
    " - **Configurable**: Tutti i parametri da config.yaml\n",
    " - **Fast evaluation**: Opzione quick eval per monitoring training\n",
    " \n",
    "  **Output e Visualizzazioni**:\n",
    " - **JSON Report**: Risultati dettagliati machine-readable\n",
    " - **Metrics Plot**: Grafico a barre delle metriche principali\n",
    " - **Comparison Grid**: Confronto visivo real vs generated\n",
    " - **Attention Heatmap**: Interpretabilità del modello\n",
    " - **Training Progress**: Dashboard evoluzione training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
