{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b320a627",
   "metadata": {},
   "source": [
    "#  PikaPikaGenerator - Model Architecture\n",
    "# \n",
    "**Progetto:** Generative Synthesis of Pokémon Sprites from Textual Descriptions  \n",
    " **Corso:** Deep Learning - Politecnico di Bari  \n",
    " **Studente:** Pasquale Alessandro Denora  \n",
    " **Professore:** Vito Walter Anelli "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425638ca",
   "metadata": {},
   "source": [
    "#  Import e Setup Iniziale\n",
    " \n",
    " Il file inizia importando le librerie essenziali per implementare il modello deep learning:\n",
    " - **torch & torch.nn**: Framework PyTorch per reti neurali\n",
    " - **transformers**: Per utilizzare modelli BERT pre-addestrati\n",
    " - **numpy**: Per operazioni numeriche e conversioni\n",
    " - **typing**: Per type hints e migliore documentazione\n",
    " - **logging**: Per tracciare informazioni durante l'esecuzione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f7054",
   "metadata": {},
   "source": [
    "#  Classe TextEncoder - Encoder Testuale con BERT\n",
    " \n",
    " La classe `TextEncoder` (righe 17-37) implementa l'encoder testuale basato su BERT:\n",
    " \n",
    " **Inizializzazione (righe 20-33)**:\n",
    " - Carica il modello BERT pre-addestrato specificato in config\n",
    " - Crea un layer di proiezione per adattare la dimensione output di BERT alla dimensione desiderata\n",
    " - Include LayerNorm e Dropout per stabilizzazione\n",
    " \n",
    " **Parametri config utilizzati**:\n",
    " - `config['model']['encoder']['model_name']`: nome del modello BERT (es: \"prajjwal1/bert-mini\")\n",
    " - `config['model']['encoder']['hidden_dim']`: dimensione delle feature testuali finali\n",
    " - `config['model']['encoder']['dropout']`: tasso di dropout per regolarizzazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encoder testuale basato su BERT con possibilità di fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.config = config['model']['encoder']\n",
    "        \n",
    "        # In questa fase carico il modello pre-addestrato\n",
    "        self.bert = AutoModel.from_pretrained(self.config['model_name'])\n",
    "        self.bert_dim = self.bert.config.hidden_size\n",
    "        \n",
    "        # Layer di per adattare la dimensione dell'output alla dimesione desiderata\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bert_dim, self.config['hidden_dim']),\n",
    "            nn.LayerNorm(self.config['hidden_dim']),\n",
    "            nn.Dropout(self.config['dropout'])\n",
    "        )\n",
    "        \n",
    "        # Opzione Facoltativa per bloccare i pesi di BERT all'inizio per evitare aggiornamenti\n",
    "        self.freeze_bert_layers(freeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea312b",
   "metadata": {},
   "source": [
    "#  Freeze/Unfreeze BERT e Forward Pass\n",
    " \n",
    " **Metodo freeze_bert_layers** (righe 38-41): Permette di congelare i parametri BERT per evitare aggiornamenti durante il training iniziale. Questo è utile per:\n",
    " - Stabilizzare il training nelle prime epoche\n",
    " - Ridurre l'uso di memoria\n",
    " - Evitare il \"catastrophic forgetting\" del modello pre-addestrato\n",
    " \n",
    " **Metodo forward** (righe 43-64): Elabora le sequenze di token attraverso BERT e proietta l'output:\n",
    " 1. Passa input_ids e attention_mask a BERT\n",
    " 2. Proietta la dimensione da BERT_dim a hidden_dim\n",
    " 3. Restituisce sia sequence_output (per ogni token) che pooled_output (solo CLS token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bert_layers(self, freeze: bool = True):\n",
    "    \"\"\"Congela o sgrava i parametri di BERT per il training\"\"\"\n",
    "    for param in self.bert.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "\n",
    "def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Forward pass attraverso l'encoder testuale\n",
    "    Returns: (sequence_output, pooled_output)\n",
    "    \"\"\"\n",
    "    # Eseguo il forward su BERT\n",
    "    outputs = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    # Prende l'output per ogni token (sequence_output))\n",
    "    sequence_output = outputs.last_hidden_state  # [batch, seq_len, bert_dim]\n",
    "    \n",
    "    # Proietta alla dimensione desiderata\n",
    "    sequence_output = self.projection(sequence_output)  # [batch, seq_len, hidden_dim]\n",
    "    \n",
    "    # ottiene  output (CLS token)\n",
    "    pooled_output = sequence_output[:, 0, :]  # [batch, hidden_dim]\n",
    "    \n",
    "    return sequence_output, pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b1f2b",
   "metadata": {},
   "source": [
    "#  Classe MultiHeadAttention - Meccanismo di Attenzione\n",
    " \n",
    " La classe `MultiHeadAttention` (righe 67-124) implementa il meccanismo di attenzione multi-head per allineare features testuali e visuali:\n",
    " \n",
    " **Inizializzazione (righe 70-82)**:\n",
    " - Definisce il numero di attention heads (8 heads fissi)\n",
    " - Crea layer di proiezione per Query, Key e Value\n",
    " - Calcola head_dim dividendo hidden_dim per num_heads\n",
    " \n",
    " **Parametri utilizzati**:\n",
    " - `config['model']['encoder']['hidden_dim']`: dimensione delle feature\n",
    " - `config['model']['encoder']['dropout']`: dropout per attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89711129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Meccanismo di multi-head attention per allineamento tra testo e immagini\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config['model']['encoder']['hidden_dim']\n",
    "        self.num_heads = 8\n",
    "        self.head_dim = self.hidden_dim // self.num_heads\n",
    "        \n",
    "        # Layer di proiezione per Query, Key, Value \n",
    "        self.q_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.k_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.v_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.out_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config['model']['encoder']['dropout'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e353d040",
   "metadata": {},
   "source": [
    "#  Forward Pass Multi-Head Attention\n",
    " \n",
    " Il metodo `forward` (righe 84-124) implementa il meccanismo di attention completo:\n",
    " \n",
    " **Step-by-step**:\n",
    " 1. **Proiezione QKV** (righe 98-100): Proietta input in Query, Key, Value e ridimensiona per multi-head\n",
    " 2. **Calcolo scores** (righe 103): Calcola attention scores con dot-product scalato\n",
    " 3. **Applicazione mask** (righe 106-108): Maschera padding tokens con valori molto negativi\n",
    " 4. **Softmax e dropout** (righe 112-113): Normalizza scores e applica dropout\n",
    " 5. **Context computation** (righe 116): Calcola context come weighted sum dei values\n",
    " 6. **Output projection** (righe 119-123): Ricompone multi-head e proietta output finale\n",
    " \n",
    " **Output**: Restituisce sia l'output finale che gli attention weights (per visualizzazione)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81052a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    self, \n",
    "    query: torch.Tensor, \n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Applica multi-head attention\n",
    "    Returns: (output, attention_weights)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = key.shape[:2]\n",
    "    \n",
    "    # Proietta e ridimensiona per multi-head attention\n",
    "    Q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    K = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    V = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "    # Calcola attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "    \n",
    "    # Applica maschera se fornita\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1).unsqueeze(1)  # Add head dimension\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Applica softmax per ottenere i attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_weights = self.dropout(attention_weights)\n",
    "    \n",
    "    # Moltiplica i weights con i valori\n",
    "    context = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    # Rifirmatta e proietta l'output\n",
    "    context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "    output = self.out_proj(context)\n",
    "    \n",
    "    # Media degli attention weights tra gli heads, utile per la visualization\n",
    "    attention_weights = attention_weights.mean(dim=1)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9547c9",
   "metadata": {},
   "source": [
    "#  Classe ResidualBlock - Blocchi Residuali per il Generatore\n",
    " \n",
    " La classe `ResidualBlock` (righe 128-165) implementa blocchi residuali con connessioni skip per il generatore CNN:\n",
    " \n",
    " **Caratteristiche (righe 131-150)**:\n",
    " - Due convoluzionali 3x3 con BatchNorm\n",
    " - Connessione shortcut per preservare gradiente\n",
    " - Opzione di upsampling per aumentare risoluzione spaziale\n",
    " - Se in_channels ≠ out_channels, adatta shortcut con conv 1x1\n",
    " \n",
    " **Forward pass (righe 152-165)**:\n",
    " - Applica due conv + batchnorm + relu\n",
    " - Se upsample=True, raddoppia dimensioni spaziali\n",
    " - Somma output con residual (connessione skip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1528aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Blocco Residuo per il generatore CNN\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, upsample: bool = False):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Connessione Shortcut\n",
    "        if in_channels != out_channels or upsample:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        if upsample:\n",
    "            self.upsample_layer = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    residual = x\n",
    "    \n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    \n",
    "    if self.upsample:\n",
    "        out = self.upsample_layer(out)\n",
    "        residual = self.upsample_layer(residual)\n",
    "    \n",
    "    residual = self.shortcut(residual)\n",
    "    out = F.relu(out + residual)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732ad45",
   "metadata": {},
   "source": [
    "#  Classe SpriteGenerator - Generatore CNN Avanzato\n",
    " \n",
    " La classe `SpriteGenerator` (righe 168-235) è il cuore della generazione di immagini:\n",
    " \n",
    " **Inizializzazione (righe 171-208)**:\n",
    " - Combina features testuali + rumore casuale\n",
    " - Parte da feature map 4x4 piccola\n",
    " - Utilizza blocchi residuali per upsampling progressivo\n",
    " - Arriva alla dimensione finale specificata in config\n",
    " \n",
    " **Parametri config utilizzati**:\n",
    " - `config['model']['encoder']['hidden_dim']`: dimensione features testuali\n",
    " - `config['model']['generator']['noise_dim']`: dimensione vettore rumore  \n",
    " - `config['model']['generator']['base_channels']`: canali base del generatore\n",
    " - `config['model']['generator']['output_size']`: dimensione finale sprite (320x320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7abf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpriteGenerator(nn.Module):\n",
    "    \"\"\"Generatore CNN avanzato con connessioni residuali\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.text_dim = config['model']['encoder']['hidden_dim']\n",
    "        self.noise_dim = config['model']['generator']['noise_dim']\n",
    "        self.base_channels = config['model']['generator']['base_channels']\n",
    "        self.output_size = config['model']['generator']['output_size']\n",
    "        \n",
    "        # Dimensione Spaziale iniziale\n",
    "        self.init_size = 4  # Dimensione iniziale dell'immagine (4x4)\n",
    "        \n",
    "        # Proiezione Iniziale\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.text_dim + self.noise_dim, self.base_channels * self.init_size * self.init_size),\n",
    "            nn.BatchNorm1d(self.base_channels * self.init_size * self.init_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a4652",
   "metadata": {},
   "source": [
    "#  Upsampling Progressivo e Generazione Finale\n",
    " \n",
    " **Blocchi residuali con upsampling** (righe 189-196): Serie di 6 blocchi che aumentano progressivamente la risoluzione:\n",
    " - 4x4 → 8x8 → 16x16 → 32x32 → 64x64 → 128x128 → 256x256\n",
    " - Ogni blocco dimezza il numero di canali per ridurre complessità\n",
    " - Usa bilinear upsampling per smooth scaling\n",
    " \n",
    " **Convoluzioni finali** (righe 199-205):\n",
    " - Riduce a 3 canali RGB\n",
    " - Usa Tanh per output in range [-1,1] (matching normalizzazione input)\n",
    " - Adaptive pooling per garantire esatta dimensione output_size\n",
    " \n",
    " **Forward pass** (righe 210-235): Gestisce l'intera pipeline di generazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocchi residuali con upsampling progressivo fino alla dimensione finale\n",
    "self.blocks = nn.ModuleList([\n",
    "    ResidualBlock(self.base_channels, self.base_channels, upsample=True),      # 4x4 -> 8x8\n",
    "    ResidualBlock(self.base_channels, self.base_channels // 2, upsample=True), # 8x8 -> 16x16\n",
    "    ResidualBlock(self.base_channels // 2, self.base_channels // 4, upsample=True), # 16x16 -> 32x32\n",
    "    ResidualBlock(self.base_channels // 4, self.base_channels // 8, upsample=True), # 32x32 -> 64x64\n",
    "    ResidualBlock(self.base_channels // 8, self.base_channels // 16, upsample=True), # 64x64 -> 128x128\n",
    "    ResidualBlock(self.base_channels // 16, 64, upsample=True), # 128x128 -> 256x256\n",
    "])\n",
    "\n",
    "# Convoluzione finale per generare immagine RGB con valori in [-1,1]\n",
    "self.final_conv = nn.Sequential(\n",
    "    nn.Conv2d(64, 32, 3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(32, 3, 3, padding=1),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "# Pooling adattivo per ottenere esttamente la dimensione dell'output_size\n",
    "self.adaptive_pool = nn.AdaptiveAvgPool2d((self.output_size, self.output_size))\n",
    "\n",
    "def forward(self, text_features: torch.Tensor, noise: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    \"\"\"Generate sprite from text features and noise\"\"\"\n",
    "    batch_size = text_features.shape[0]\n",
    "    \n",
    "    # Se il rumore non viene fornito, viene generato casualmente\n",
    "    if noise is None:\n",
    "        noise = torch.randn(batch_size, self.noise_dim, device=text_features.device)\n",
    "    \n",
    "    # Concatena features testuali e rumore\n",
    "    combined = torch.cat([text_features, noise], dim=1)\n",
    "    \n",
    "    # Proiezione iniziale e reshape in feature map\n",
    "    x = self.fc(combined)\n",
    "    x = x.view(batch_size, self.base_channels, self.init_size, self.init_size)\n",
    "    \n",
    "    # Unshampling progressivo tramite blocchi residuali\n",
    "    for block in self.blocks:\n",
    "        x = block(x)\n",
    "    \n",
    "    # Convoluzioni finali per immagine RGB\n",
    "    x = self.final_conv(x)\n",
    "    \n",
    "    # Pooling adattivo per ottenere la dimensione finale desiderata\n",
    "    x = self.adaptive_pool(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d21b5",
   "metadata": {},
   "source": [
    "#  Classe PikaPikaGenerator - Modello Completo\n",
    " \n",
    " La classe `PikaPikaGenerator` (righe 238-357) orchestra tutti i componenti:\n",
    " \n",
    " **Inizializzazione (righe 241-251)**:\n",
    " - Crea TextEncoder, MultiHeadAttention, SpriteGenerator\n",
    " - Carica il tokenizer per preprocessing testo\n",
    " - Integra tutti i componenti in un'unica architettura\n",
    " \n",
    " **Forward pass completo** (righe 253-283):\n",
    " 1. Encode del testo attraverso BERT\n",
    " 2. Applica self-attention per raffinare features\n",
    " 3. Genera sprite usando il generatore CNN\n",
    " 4. Restituisce immagine + attention weights per analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PikaPikaGenerator(nn.Module):\n",
    "    \"\"\"Modello completo per text-to sprite generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Componenti iniziali\n",
    "        self.text_encoder = TextEncoder(config)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.generator = SpriteGenerator(config)\n",
    "        \n",
    "        # Carica il Tokenizer per la tokenizzazione del testo\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config['model']['encoder']['model_name'])\n",
    "\n",
    "def forward(\n",
    "    self, \n",
    "    input_ids: torch.Tensor, \n",
    "    attention_mask: torch.Tensor,\n",
    "    noise: Optional[torch.Tensor] = None\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Forward pass attraverso il model completo\n",
    "    Restituisce un dizionario con lle immagini generate e gli attention weights\n",
    "    \"\"\"\n",
    "    # Encoder testuale\n",
    "    sequence_output, pooled_output = self.text_encoder(input_ids, attention_mask)\n",
    "    \n",
    "    # Applica self-attention per rifinire le features testuali\n",
    "    attended_features, attention_weights = self.attention(\n",
    "        query=pooled_output.unsqueeze(1),\n",
    "        key=sequence_output,\n",
    "        value=sequence_output,\n",
    "        mask=attention_mask\n",
    "    )\n",
    "    \n",
    "    attended_features = attended_features.squeeze(1)  # Rimuove la dimensione del sequence length\n",
    "    \n",
    "    # Generazione sprite\n",
    "    generated_image = self.generator(attended_features, noise)\n",
    "    \n",
    "    return {\n",
    "        'generated_image': generated_image,\n",
    "        'attention_weights': attention_weights,\n",
    "        'text_features': attended_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250e92b",
   "metadata": {},
   "source": [
    "#  Metodi di Utilità per Generazione e Visualizzazione\n",
    " \n",
    " **Metodo generate** (righe 285-317): Interfaccia semplificata per generare sprite da testo:\n",
    " - Tokenizza automaticamente il testo input\n",
    " - Gestisce device placement (CPU/GPU)\n",
    " - Denormalizza output da [-1,1] a [0,255] per visualizzazione\n",
    " - Restituisce numpy array pronto per essere mostrato\n",
    " \n",
    " **Metodo get_attention_visualization** (righe 319-357): Per analisi dei pattern di attenzione:\n",
    " - Restituisce immagine generata + tokens + attention weights\n",
    " - Utile per capire quali parole influenzano parti specifiche dell'immagine\n",
    " - Essenziale per debugging e interpretabilità del modello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    self, \n",
    "    text: str, \n",
    "    noise: Optional[torch.Tensor] = None,\n",
    "    device: str = 'cpu'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Genera sprite dalle descrizioni testuali\"\"\"\n",
    "    self.eval()\n",
    "    \n",
    "    # Tokenizza il testo\n",
    "    encoding = self.tokenizer(\n",
    "        text,\n",
    "        max_length=self.config['model']['encoder']['max_length'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Sposta i tensori sul dispositivo specificato\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = self.forward(input_ids, attention_mask, noise)\n",
    "        generated_image = outputs['generated_image']\n",
    "    \n",
    "    # Converte l'immagine generata in un array NumPy\n",
    "    image = generated_image.squeeze(0).cpu()\n",
    "    image = (image + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_attention_visualization(\n",
    "    self, \n",
    "    text: str,\n",
    "    device: str = 'cpu'\n",
    ") -> Tuple[np.ndarray, List[str], np.ndarray]:\n",
    "    \"\"\"Ottiene gli attention weights e i tokens per visualizzazione\"\"\"\n",
    "    self.eval()\n",
    "    \n",
    "    # Tokenizza il testo\n",
    "    encoding = self.tokenizer(\n",
    "        text,\n",
    "        max_length=self.config['model']['encoder']['max_length'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Ottiene i tokens\n",
    "    tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    \n",
    "    # Sposta i tensori sul dispositivo specificato\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = self.forward(input_ids, attention_mask)\n",
    "        attention_weights = outputs['attention_weights']\n",
    "        generated_image = outputs['generated_image']\n",
    "    \n",
    "    # Processa gli attention weights\n",
    "    attention_weights = attention_weights.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Converte l'immagine generata in un array NumPy\n",
    "    image = generated_image.squeeze(0).cpu()\n",
    "    image = (image + 1) / 2\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    return image, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b0a73",
   "metadata": {},
   "source": [
    "#  Classe Discriminator - Valutazione Qualità Immagini\n",
    " \n",
    " La classe `Discriminator` (righe 360-406) valuta la qualità delle immagini generate:\n",
    " \n",
    " **Architettura (righe 367-390)**:\n",
    " - Progressive downsampling: 320x320 → 160x160 → 80x80 → 40x40 → 20x20 → 10x10\n",
    " - Ogni blocco raddoppia i canali e dimezza risoluzione spaziale\n",
    " - Global average pooling per ridurre a feature vector\n",
    " - Classificatore finale per score di realismo\n",
    " \n",
    " **Metodo _make_block** (righe 392-399): Crea blocchi convoluzionali standard:\n",
    " - Conv2d con stride 2 per downsampling\n",
    " - BatchNorm per stabilizzazione\n",
    " - LeakyReLU per non-linearità (meglio per discriminatori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator per la valutazione della qualità delle immagini generate\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.input_size = config['model']['generator']['output_size']\n",
    "        \n",
    "        # Progressive downsampling con blocchi convoluzionali\n",
    "        self.blocks = nn.Sequential(\n",
    "            # 215x215 -> 107x107\n",
    "            self._make_block(3, 64, downsample=True),\n",
    "            # 107x107 -> 53x53\n",
    "            self._make_block(64, 128, downsample=True),\n",
    "            # 53x53 -> 26x26\n",
    "            self._make_block(128, 256, downsample=True),\n",
    "            # 26x26 -> 13x13\n",
    "            self._make_block(256, 512, downsample=True),\n",
    "            # 13x13 -> 6x6\n",
    "            self._make_block(512, 512, downsample=True),\n",
    "        )\n",
    "        \n",
    "        # Media pooling per ridurre le dimensioni\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Classificatore finale\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "def _make_block(self, in_channels: int, out_channels: int, downsample: bool = True):\n",
    "    \"\"\"Crea un blocco convoluzionale con opzioni di downsampling\"\"\"\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, 4, 2 if downsample else 1, 1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    ]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward pass attraverso il discriminatore\"\"\"\n",
    "    features = self.blocks(x)\n",
    "    features = self.global_pool(features).view(features.size(0), -1)\n",
    "    output = self.classifier(features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487babf",
   "metadata": {},
   "source": [
    "#  Funzione create_model - Factory Pattern\n",
    " \n",
    " La funzione `create_model` (righe 409-433) è la factory function per creare e inizializzare il modello:\n",
    " \n",
    " **Inizializzazione pesi** (righe 413-423):\n",
    " - Xavier uniform per Conv2d, ConvTranspose2d, Linear\n",
    " - Zero bias per tutti i layer\n",
    " - Pesi a 1 e bias a 0 per BatchNorm\n",
    " - Applica solo al generatore (encoder BERT già pre-addestrato)\n",
    " \n",
    " **Logging informazioni** (righe 426-433):\n",
    " - Conta parametri totali e trainable\n",
    " - Utile per monitorare complessità modello\n",
    " - Aiuta a diagnosticare problemi di memoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config: Dict) -> PikaPikaGenerator:\n",
    "    \"\"\"Funzione per creare e inizializzare il modello PikaPikaGenerator\"\"\"\n",
    "    model = PikaPikaGenerator(config)\n",
    "    \n",
    "    # Inizializza i weights\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    model.generator.apply(init_weights)\n",
    "    \n",
    "    # Logging delle informazioni sul modello\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    logger.info(f\"Created PikaPikaGenerator model\")\n",
    "    logger.info(f\"Total parameters: {total_params:,}\")\n",
    "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abb137",
   "metadata": {},
   "source": [
    "#  Script di Test dell'Architettura\n",
    " \n",
    " Il blocco finale (righe 436-484) permette di testare l'architettura quando si esegue il file direttamente:\n",
    " \n",
    " **Test completi eseguiti**:\n",
    " 1. **Caricamento config**: Prova a caricare config.yaml, fallback a config di test\n",
    " 2. **Creazione modello**: Testa l'istanziazione di tutti i componenti\n",
    " 3. **Forward pass**: Verifica che il forward pass funzioni senza errori\n",
    " 4. **Shape checking**: Controlla che le dimensioni output siano corrette\n",
    " 5. **Generazione testo**: Testa l'interfaccia semplificata generate()\n",
    " \n",
    " Questo è essenziale per verificare che l'architettura sia implementata correttamente prima del training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2971530",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test di creazione del modello e forward pass\n",
    "    import yaml\n",
    "    \n",
    "    # Esempio config per il testing\n",
    "    test_config = {\n",
    "        'model': {\n",
    "            'encoder': {\n",
    "                'model_name': 'prajjwal1/bert-mini',\n",
    "                'hidden_dim': 256,\n",
    "                'max_length': 128,\n",
    "                'dropout': 0.1\n",
    "            },\n",
    "            'generator': {\n",
    "                'noise_dim': 100,\n",
    "                'base_channels': 512,\n",
    "                'output_size': 215\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open('configs/config.yaml', 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config file not found, using test config\")\n",
    "        config = test_config\n",
    "    \n",
    "    model = create_model(config)\n",
    "    \n",
    "    # Test forward pass\n",
    "    batch_size = 2\n",
    "    seq_len = 128\n",
    "    \n",
    "    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "    attention_mask = torch.ones(batch_size, seq_len)\n",
    "    \n",
    "    print(\"Testing forward pass...\")\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    \n",
    "    print(f\"Generated image shape: {outputs['generated_image'].shape}\")\n",
    "    print(f\"Attention weights shape: {outputs['attention_weights'].shape}\")\n",
    "    \n",
    "    # Generazione del testo\n",
    "    print(\"Testing text generation...\")\n",
    "    test_text = \"A small yellow electric mouse Pokemon with red cheeks\"\n",
    "    image = model.generate(test_text)\n",
    "    print(f\"Generated sprite shape: {image.shape}\")\n",
    "    print(\"Architecture test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d703c",
   "metadata": {},
   "source": [
    "#  Riepilogo Parametri Config Utilizzati\n",
    " \n",
    " Il file `architecture.py` utilizza questi parametri dal file `config.yaml`:\n",
    " \n",
    " **Sezione `model.encoder`**:\n",
    " - `model_name`: Modello BERT pre-addestrato (\"prajjwal1/bert-mini\")\n",
    " - `hidden_dim`: Dimensione feature testuali (512 nel tuo config)  \n",
    " - `max_length`: Lunghezza massima sequenze (128)\n",
    " - `dropout`: Tasso dropout per regolarizzazione (0.1)\n",
    " - `attention_heads`: Numero heads per attention (12, non usato nel file mostrato)\n",
    " - `attention_layers`: Numero layer attention (6, non usato nel file mostrato)\n",
    " \n",
    "# **Sezione `model.generator`**:\n",
    " - `noise_dim`: Dimensione vettore rumore casuale (256)\n",
    " - `base_channels`: Canali base del generatore (768)\n",
    " - `output_size`: Dimensione finale sprite (320x320)\n",
    " - `text_dim`: Dimensione feature testuali input (512)\n",
    " - `activation`: Funzione attivazione (\"leaky_relu\")\n",
    " - `normalization`: Tipo normalizzazione (\"spectral\")\n",
    " - `use_self_attention`: Se usare self-attention (true)\n",
    " \n",
    " **Architettura Modulare**:\n",
    " Il design permette di facilmente:\n",
    " - Modificare dimensioni attraverso config\n",
    " - Sostituire componenti (es. diverso encoder BERT)\n",
    "- Aggiungere nuove funzionalità senza rompere esistente\n",
    " - Testare diverse configurazioni rapidly\n",
    "\n",
    "\n",
    "#  Conclusioni - Architettura PikaPikaGenerator\n",
    " \n",
    " L'architettura implementata in `architecture.py` è un modello encoder-decoder avanzato con le seguenti caratteristiche:\n",
    " \n",
    "#  **Componenti Principali**:\n",
    " 1. **TextEncoder**: BERT pre-addestrato + proiezione per encoding testuale\n",
    " 2. **MultiHeadAttention**: Meccanismo attenzione per allineamento testo-immagine\n",
    " 3. **SpriteGenerator**: CNN generativa con blocchi residuali e upsampling progressivo\n",
    " 4. **Discriminator**: Valutatore qualità immagini per training adversarial\n",
    " \n",
    "\n",
    "#  **Pipeline di Generazione**:\n",
    " 1. **Input**: Descrizione testuale Pokemon\n",
    " 2. **Encoding**: BERT → proiezione → attention-refined features  \n",
    " 3. **Generation**: Features + rumore → upsampling progressivo → sprite 320x320\n",
    " 4. **Output**: Immagine RGB + attention weights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
